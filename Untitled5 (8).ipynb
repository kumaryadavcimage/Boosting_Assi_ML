{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1592cab9-3116-4581-8eef-24b353325e81",
   "metadata": {},
   "source": [
    "#### Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecf07d0-fe77-44dc-8bbd-2c0a9dba15e1",
   "metadata": {},
   "source": [
    "#### solve\n",
    "Gradient Boosting Regression is a machine learning technique used for regression tasks, where the goal is to predict a continuous outcome variable based on one or more input features. It belongs to the family of ensemble learning methods, which combine the predictions of multiple individual models to improve overall performance.\n",
    "\n",
    "Here's how the process generally works:\n",
    "\n",
    "- Initialize the model: The process starts with an initial model, often a simple one like a single decision tree, which makes initial predictions for the target variable.\n",
    "- Compute residuals: The errors (residuals) between the actual target values and the predictions of the current model are computed.\n",
    "- Fit a new model to the residuals: A new model (typically a decision tree) is trained to predict the residuals of the current model. This model is trained on the residuals, aiming to capture the patterns or relationships that the previous model failed to learn.\n",
    "- Update the ensemble: The new model's predictions are combined with the predictions of the previous models, and the ensemble's predictions are updated. This update process typically involves adding the predictions of the new model to the predictions of the existing models, but with a smaller weight to prevent overfitting.\n",
    "- Iterate: Steps 2-4 are repeated iteratively for a predefined number of iterations (or until a certain stopping criterion is met), with each new model focusing on reducing the errors that remain after the predictions of the existing models are taken into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0d7d4a-57ce-4c0e-b00e-9ed55ccc2190",
   "metadata": {},
   "source": [
    "#### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03619ef6-f043-4f55-863e-548d37652490",
   "metadata": {},
   "source": [
    "#### solve\n",
    "Here's a simple implementation of gradient boosting for regression using Python and NumPy. We'll train the model on a small synthetic dataset and evaluate its performance using mean squared error (MSE) and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c39fb86-ad65-4d3e-a753-e3984779916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.initial_prediction = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize with the mean of the target values\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        residuals = y - self.initial_prediction\n",
    "\n",
    "        # Fit base model\n",
    "        base_model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "        base_model.fit(X, residuals)\n",
    "        self.models.append(base_model)\n",
    "\n",
    "        # Update predictions iteratively\n",
    "        for _ in range(1, self.n_estimators):\n",
    "            prediction = self.predict(X)\n",
    "            residuals = y - prediction\n",
    "            base_model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            base_model.fit(X, residuals)\n",
    "            self.models.append(base_model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.full(X.shape[0], self.initial_prediction)\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# Generate synthetic dataset\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X.squeeze() + np.random.randn(100)  # True relationship: y = 2*X + noise\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test = X[:80], X[80:]\n",
    "y_train, y_test = y[:80], y[80:]\n",
    "\n",
    "# Instantiate and train the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = np.mean((y_test - y_pred) ** 2)\n",
    "r_squared = 1 - np.sum((y_test - y_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R-squared:\", r_squared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94338a1-ae83-4d0f-ba1b-46bcf3a6861d",
   "metadata": {},
   "source": [
    "####\n",
    "In this implementation:\n",
    "\n",
    "- We define a GradientBoostingRegressor class with fit and predict methods.\n",
    "- In the fit method, we initialize the model with the mean of the target values and iteratively fit decision trees to the residuals.\n",
    "- We use a synthetic dataset where the relationship between the feature (X) and the target (y) is linear with some Gaussian noise added.\n",
    "- We evaluate the model's performance using MSE and R-squared on a held-out test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b12796-0d0e-475b-822d-52100a3d05fe",
   "metadata": {},
   "source": [
    "#### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb87803-94b8-4593-bf04-da9be134631e",
   "metadata": {},
   "source": [
    "#### solve\n",
    "Let's perform a grid search over different combinations of hyperparameters to find the best combination that optimizes the performance of the model. We'll vary the learning rate, the number of trees (estimators), and the maximum depth of each tree. Then, we'll evaluate the model's performance using mean squared error (MSE) on a held-out validation set.\n",
    "\n",
    "Here's how you can implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876cd285-6310-4a07-8aa2-da06425b7d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "best_params = None\n",
    "best_mse = float('inf')\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    gb_regressor = GradientBoostingRegressor(**params)\n",
    "    gb_regressor.fit(X_train, y_train)\n",
    "    y_pred_val = gb_regressor.predict(X_test)\n",
    "    mse_val = mean_squared_error(y_test, y_pred_val)\n",
    "    \n",
    "    if mse_val < best_mse:\n",
    "        best_mse = mse_val\n",
    "        best_params = params\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best Mean Squared Error (MSE):\", best_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c43d1-4703-40ba-adc2-f5ed7d475711",
   "metadata": {},
   "source": [
    "####\n",
    "In this code:\n",
    "\n",
    "- We define a grid of hyperparameters to search over, including learning rate, number of trees (estimators), and maximum depth of each tree.\n",
    "- We iterate over all combinations of hyperparameters using ParameterGrid from scikit-learn.\n",
    "- For each combination of hyperparameters, we train a GradientBoostingRegressor model, evaluate its performance on the validation set using mean squared error (MSE), and keep track of the best combination of hyperparameters that minimizes MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba58263-e198-4524-b4cc-d8b444ea0e08",
   "metadata": {},
   "source": [
    "#### Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d4d220-4f07-4fc9-9f9c-ba863ed991b5",
   "metadata": {},
   "source": [
    "#### solve\n",
    "In Gradient Boosting, a weak learner refers to a base model that performs slightly better than random guessing but is still relatively simple. Typically, decision trees are used as weak learners in gradient boosting, although other types of models can also be used.\n",
    "\n",
    "Here are some characteristics of weak learners in Gradient Boosting:\n",
    "- Low Complexity: Weak learners are usually simple models with low complexity, such as decision trees with shallow depth or stumps (trees with only one split).\n",
    "- Slight Better Than Random Guessing: Weak learners should perform slightly better than random guessing on the training data. They might have a high bias but low variance.\n",
    "- Sequential Improvement: In Gradient Boosting, weak learners are trained sequentially, with each new weak learner focusing on capturing the patterns or relationships that the previous weak learners failed to learn. This sequential training process allows the ensemble to iteratively reduce the errors made by the previous weak learners.\n",
    "- Contribution to the Ensemble: While individual weak learners may not perform well on their own, when combined through the boosting process, they contribute to the overall predictive power of the ensemble model. By focusing on the difficult examples that the previous models struggled with, each new weak learner improves the ensemble's performance.\n",
    "- Regularization: Weak learners help prevent overfitting in the ensemble model. Since they are simple models, they are less prone to overfitting, and the boosting process itself provides a form of regularization by penalizing misclassifications from previous weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e1549c-134a-4c24-8633-7ef38eea4436",
   "metadata": {},
   "source": [
    "#### Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39cd844-a53e-4345-8c27-dea031d87663",
   "metadata": {},
   "source": [
    "#### solve\n",
    "The intuition behind the Gradient Boosting algorithm can be understood through the following key concepts:\n",
    "\n",
    "- Ensemble Learning: Gradient Boosting is an ensemble learning method, which means it combines the predictions of multiple individual models (weak learners) to improve overall performance. Instead of relying on a single complex model, ensemble methods leverage the diversity of multiple models to make more accurate predictions.\n",
    "- Sequential Learning: Gradient Boosting builds an ensemble of models sequentially. It starts with an initial model (often a simple one), makes predictions on the training data, and then focuses on improving the predictions of the ensemble by sequentially adding new models.\n",
    "- Gradient Descent Optimization: The \"gradient\" in Gradient Boosting refers to the gradient of the loss function (typically the mean squared error for regression tasks) with respect to the ensemble's predictions. Gradient Boosting minimizes this loss function by iteratively fitting new models to the residuals (errors) of the previous models. Each new model is trained to predict the negative gradient (i.e., the direction in which the loss decreases the most) of the loss function.\n",
    "- Gradient Boosting with Trees: In practice, decision trees are often used as weak learners in Gradient Boosting. Each new decision tree is fitted to the negative gradient of the loss function, which effectively captures the patterns or relationships that the previous trees failed to learn. By iteratively adding new trees, Gradient Boosting constructs a powerful ensemble model that combines the strengths of multiple decision trees.\n",
    "- Additive Training: The predictions of the ensemble are obtained by summing the predictions of all the individual models. At each iteration, a new model is added to the ensemble, and its predictions are added to the predictions of the existing ensemble, with a certain weight (learning rate) to control the contribution of each model.\n",
    "- Regularization: Gradient Boosting provides a form of regularization by penalizing misclassifications or errors made by the ensemble's predictions. This helps prevent overfitting and improves the generalization performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8132969-7505-4d5c-a7b0-71a0c88b9cc9",
   "metadata": {},
   "source": [
    "#### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6781efa0-87b0-4359-8338-d9a62db7831a",
   "metadata": {},
   "source": [
    "#### solve\n",
    "- Gradient Boosting builds an ensemble of weak learners (typically decision trees) sequentially, with each new weak learner focusing on capturing the errors (residuals) made by the previous learners. Here's how the Gradient Boosting algorithm constructs the ensemble:\n",
    "- Initialize the Ensemble: Gradient Boosting starts with an initial prediction for each sample in the training data. This initial prediction can be a simple value, such as the mean of the target variable for regression tasks or the log odds for classification tasks.\n",
    "- Compute Residuals: The initial predictions are subtracted from the actual target values to compute the residuals (errors). These residuals represent the errors made by the initial model and serve as the target for subsequent models to correct.\n",
    "- Fit a Weak Learner to the Residuals: A weak learner (typically a decision tree) is trained to predict the residuals of the current ensemble's predictions. The goal of this weak learner is to capture the patterns or relationships in the data that the previous models failed to learn.\n",
    "- Update Ensemble Predictions: The predictions of the new weak learner are added to the predictions of the existing ensemble, with a certain weight (learning rate) to control the contribution of each model. This update process is additive, meaning the predictions of each weak learner are added to the ensemble's predictions, gradually improving the overall predictions.\n",
    "- Compute New Residuals: The new predictions of the ensemble are subtracted from the actual target values to compute updated residuals. These updated residuals represent the errors that remain after the predictions of the current weak learner are taken into account.\n",
    "- Iterate: Steps 3-5 are repeated iteratively for a predefined number of iterations (number of trees) or until a certain stopping criterion is met. Each new weak learner is trained to predict the residuals of the current ensemble's predictions, focusing on reducing the errors that remain after the predictions of the existing models are considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91173f7c-bc90-45af-b645-d036bce377ad",
   "metadata": {},
   "source": [
    "#### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c4c1da-6800-40b2-a17a-3244e0cfa515",
   "metadata": {},
   "source": [
    "#### solve\n",
    "Constructing the mathematical intuition behind the Gradient Boosting algorithm involves understanding the key concepts and steps involved in the algorithm. Here's a breakdown of the steps involved:\n",
    "\n",
    "- Loss Function: Gradient Boosting aims to minimize a loss function, which measures the difference between the model's predictions and the actual target values. For regression tasks, the loss function is typically the mean squared error (MSE), while for classification tasks, it can be the cross-entropy loss or another appropriate loss function.\n",
    "- Initialize Model: Gradient Boosting starts with an initial model, often a simple one such as a constant value (for regression) or the log odds (for classification). This initial model makes initial predictions for all samples in the dataset.\n",
    "- Compute Residuals: The residuals are the differences between the actual target values and the predictions of the current ensemble. In the case of the initial model, the residuals are simply the differences between the target values and the initial predictions.\n",
    "- Fit Weak Learner to Residuals: A weak learner (typically a decision tree) is trained to predict the residuals of the current ensemble. The weak learner is trained using a gradient descent optimization algorithm to minimize the loss function.\n",
    "- Update Ensemble Predictions: The predictions of the weak learner are added to the predictions of the current ensemble, with a certain weight (learning rate) to control the contribution of each model. This update process is additive, meaning the predictions of each weak learner are added to the ensemble's predictions.\n",
    "- Compute New Residuals: The new predictions of the ensemble are subtracted from the actual target values to compute updated residuals. These updated residuals represent the errors that remain after the predictions of the current weak learner are taken into account.\n",
    "- Iterate: Steps 4-6 are repeated iteratively for a predefined number of iterations (number of trees) or until a certain stopping criterion is met. Each new weak learner is trained to predict the residuals of the current ensemble's predictions, focusing on reducing the errors that remain after the predictions of the existing models are considered.\n",
    "- Final Prediction: The final prediction of the ensemble is obtained by summing the predictions of all the individual weak learners. This final prediction represents the ensemble's prediction for each sample in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83c007-884b-40ee-9b89-cb110aa481dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea6f63-028a-48ba-9156-f64585b8c2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
